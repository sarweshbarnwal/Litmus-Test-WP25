{"cells":[{"cell_type":"markdown","source":["## **Linear Regression**\n","We will use Linear regression for predicting house prices\n","\n","We are using a Kaggle dataset- https://www.kaggle.com/harlfoxem/housesalesprediction"],"metadata":{"id":"FSRalact4xj4"}},{"cell_type":"code","source":["# Lets import required Libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n"],"metadata":{"id":"J8MRCcJY2btt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Dataset Preparation**"],"metadata":{"id":"1MzAhMmE6UHE"}},{"cell_type":"code","source":["# Execute this cell for loading dataset in a pandas dataframe\n","\n","from IPython.display import clear_output\n","!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=16x6-8Znn2T50zFwVvKlzsdN7Jd1hpjct' -O Linear_regression_dataset\n","\n","data_df = pd.read_csv(\"Linear_regression_dataset\")"],"metadata":{"id":"LiWI-2py554R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lets have a quick Look at dataset\n","\n","print(\"(No of rows, No of Columns) = \",data_df.shape)\n","data_df.head()"],"metadata":{"id":"TAodxbYX7AKf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So there are **19** features (of course we will not use id as feature :) ), and 1 variable to predict(price)\n","\n","But note that the **date** column contain strings so first we will remove T00.. part from it and than convert it to numpy array."],"metadata":{"id":"gsJaooGZ7pUV"}},{"cell_type":"code","source":["data_df[\"date\"] = data_df[\"date\"].str.replace(\"T000000\",\"\")                                      # Remove T000000 part from data column. Hint: search about .str.replace() method. :)\n","data_df = data_df.drop(\"id\", axis = 1)\n","data_array = data_df.to_numpy()                                             # Create a numpy array which does not have \"id\" field\n","assert (data_array.shape == (21613,20))\n","\n","data_df.head()"],"metadata":{"id":"FNFNf3jT7oxW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now the next task is **normalization**.\n","\n","We will scale each column of dataset by x -> (x-u)/s\n","\n","where u is mean(x), and s is standard deviation of u"],"metadata":{"id":"xsBZxZ4x-oBR"}},{"cell_type":"code","source":["data_df = data_df.astype(\"float32\")\n","mean = np.array(data_df.agg(\"mean\"))                                  # this should be an array, each entry should be mean of a column\n","sd = np.array(data_df.agg(\"std\"))                                   # this should be an array, each entry should be standard deviation of a column\n","\n","data_array_norm = (data_array.astype(\"float32\") - mean)/sd\n","\n","print(data_array_norm.shape)"],"metadata":{"id":"u7GZV-0T_zCy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The last step is to make train and test dataset and to create seperate vector for price"],"metadata":{"id":"VCQTrNIgAlPv"}},{"cell_type":"code","source":["labels = np.array(data_df[\"price\"])                                                                                                           # extract the price column from data\n","\n","x_array_norm = np.delete(data_array_norm,1,1)                                                                                                 # delete the price column from data_array_norm. Hint: use np.delete()\n","\n","x_train, x_test, y_train, y_test = train_test_split(x_array_norm,labels,test_size=0.15,random_state=42,shuffle=True)    # splitting data into test and train set.\n","\n","print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"],"metadata":{"id":"dJyX5QOFBRg5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Loss and gradient descent**\n","We will use mean squared error(MSE) as loss\n","\n","Use the gradient descent algorithm which you learned from tutorials\n","\n","Your task is to complete the following functions"],"metadata":{"id":"iAdW-22ZDcdU"}},{"cell_type":"code","source":["def loss(y_pred,y_true):\n","  \"\"\"\n","  input:\n","  y_pred = [array] predicted value of y\n","  y_true = [array] ground truth\n","\n","  output:\n","  mse: [scalar] the MES loss\n","  \"\"\"\n","  mse = sum((y_pred-y_true)**2)/len(y_true)                # fill code here\n","\n","  return mse"],"metadata":{"id":"ufoIQOpeEFQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def y(x,a,b):\n","  \"\"\"\n","  This function should return predicted value of y = ax+b\n","  input:\n","  x: [array] the feature vector of shape (m,n)\n","  a: [array] weights of shape (n,)\n","  b: [scalar] bias\n","\n","  output:\n","  y_pred: [array] predicted value of y of shape (m,)\n","  \"\"\"\n","\n","  m,n = x.shape\n","  y_pred = x@a + b                   # fill code here\n","\n","  assert(y_pred.shape == (m,))\n","  return y_pred"],"metadata":{"id":"a6LogNz5E28X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gradient(x,a,b,y_true):\n","  \"\"\"\n","  This function shoud return gradient of loss\n","  input:\n","  x: [array] the feature vector of shape (m,n)\n","  a: [array] weights of shape (n,)\n","  b: [scalar] bias\n","  y_true: [array] ground truth of shape (m,)\n","\n","  output:\n","  grad: [tuple] a tuple (derivative with respect to a[array of shape(n,)], derivative with respect to b[scalar])\n","  \"\"\"\n","  m,n = x.shape\n","  yp = y(x,a,b)\n","  yp = yp.reshape(-1)\n","  y_true = y_true.reshape(-1)\n","  da = 2*x.T@(yp-y_true)/len(y_true)              # write code to calculate derivative of loss with respect to a\n","  db = 2*sum((yp-y_true))/len(y_true)              # write code to calculate derivative of loss with respect to b\n","  da = da.reshape(-1)\n","  assert(da.shape ==(n,))\n","  return (da,db)"],"metadata":{"id":"lYnPROu8Gxwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gradient_descent(x,y_true,learning_rate=0.01,epochs = 10):\n","  \"\"\"\n","  This function perfroms gradient descent and minimizes loss\n","  input:\n","  x: [array] the feature vector of shape (m,n)\n","  y_true: [array] ground truth of shape (m,)\n","\n","  output:\n","  loss: [array] of size (epochs,)\n","  weights: [tuple] (a,b)\n","  \"\"\"\n","  m,n = x.shape\n","  loss_mse = []                                 # initialize empty list to store loss\n","  a = np.zeros(n)                                       # initialize a- weights and b- bias\n","  b = 0.0\n","\n","  for i in range(epochs):\n","    # calculate derivative using gradient() function\n","    # apply gradient descent now to update a and b\n","\n","    l_mse = loss(y_pred = y(x,a,b), y_true = y_true)                                # calculate loss at this point\n","    loss_mse.append(l_mse)\n","\n","    da, db = gradient(x,a,b,y_true)\n","    a-=learning_rate*da\n","    b-=learning_rate*db\n","\n","    print(\"Epoch \",i+1,\" Completed!\",\"loss = \",l_mse)\n","\n","  print(\"Training completed!!\")\n","\n","  assert(a.shape==(n,))\n","\n","  return (loss_mse,a,b)"],"metadata":{"id":"km_z3ojKKQdj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Training**"],"metadata":{"id":"VsR5XLl_WVu4"}},{"cell_type":"code","source":["epochs = 100              # tweak this!!!\n","learn_rate = 0.03          # choose learning rate wisely otherwise loss may diverge!!\n","\n","train_loss,a,b = gradient_descent(x_train,y_train,learning_rate=learn_rate,epochs = epochs)"],"metadata":{"id":"5A9mqkqLWU27"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Evaluation and Visualization**\n","Lets plot how loss varies with epochs\n"],"metadata":{"id":"TDH-7NHQT50f"}},{"cell_type":"code","source":["test_loss = loss(y(x_test,a,b), y_test)\n","\n","print(\"Loss on test data = \",test_loss)\n","\n","# Visualization of loss\n","\n","plt.plot(range(1,epochs+1), train_loss)                   # plot loss versus epochs\n","plt.title(\"Training Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.show()"],"metadata":{"id":"d7JRB_nJUEkV"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"1dBCIHsuBjEOG5VX94u0iECX1GH0pLR0S","timestamp":1766079679414},{"file_id":"1VH6yrglT6IFLzpiHeJZxA7Oun2s3IQ-D","timestamp":1765540175823},{"file_id":"1d4hX-V-dgs_wOqUDY3ht4wKlRyC8VmGp","timestamp":1733569901466},{"file_id":"1ZSXd0svqcWXqfyC_RXfxQYfdnDzV8VyS","timestamp":1715937933644},{"file_id":"1G0HA0O3qU5M9qFeZP4v3QhLyPdAWReSV","timestamp":1642843495024},{"file_id":"13GsIL6AVXN4pFFsc2zYdVuFVhJ2dnjp4","timestamp":1642755707968}]}},"nbformat":4,"nbformat_minor":0}